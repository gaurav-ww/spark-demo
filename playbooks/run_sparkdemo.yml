# this playbook downloads the latest SparkDemo artifact from Nexus and submit the SparkDemo job

- name: Download the SparkDemo artifact
  hosts: all
  any_errors_fatal: true
  become: yes
  become_user: hdfs
  gather_facts: False
  vars:
    nexus:
      host: "localhost"
      # username : "tower"
      # userpassword : "tower"
    repository: "http://{{ nexus.host }}:8081/nexus/content/repositories/snapshots"
    groupId: "com.whishworks.spark"
    artifact_id: "sparkdemo"
    version: "1.0-SNAPSHOT"
    extension: "jar"
    demoClass: "SampleJob"
    artifact_path: "/tmp/{{ artifact_id }}-{{ version }}.{{ extension }}"
    result_file: "/tmp/sparksubmit-{{demoClass}}-{{ artifact_id }}-{{ version }}.output"
  tasks:
    - name: Update repository URL ( for releases)
      set_fact: repository="http://{{ nexus.host }}:8081/nexus/content/repositories/releases"
      when: "'SNAPSHOT' not in version"

    - name: Remove the existing artifact file(s)
      file: path="{{ item }}" state=absent
      with_items:
        - "{{ artifact_path }}"
        - "{{ result_file }}"
      become : yes

    - name: Download the artifact from Nexus
      maven_artifact: group_id="{{ groupId }}" artifact_id="{{ artifact_id }}" repository_url="{{ repository }}" username="{{ nexus.username | default(omit)}}" password="{{ nexus.userpassword  | default(omit)}}" version="{{ version }}" classifier="{{ classifier | default(omit)}}" extension="{{ extension }}" validate_certs=no dest="{{ artifact_path }}" version="{{ version }}"

- name: Run the SparkDemo - {{ demoClass }}
  hosts: all
  any_errors_fatal: true
  become: yes
  become_user: hdfs
  gather_facts: False
  vars:
    groupId: "com.whishworks.spark"
    artifact_id: "sparkdemo"
    version: "1.0-SNAPSHOT"
    extension: "jar"
    demoClass: "SampleJob"
    artifact_path: "/tmp/{{ artifact_id }}-{{ version }}.{{ extension }}"
    result_file: "/tmp/sparksubmit-{{demoClass}}-{{ artifact_id }}-{{ version }}.output"
  tasks:
    - block:
        - name: Submit the Spark job
          shell: "spark-submit --master yarn --class {{ groupId + '.' + demoClass}} {{ artifact_path }} > {{ result_file }}"
          args:
            chdir: "/tmp/"
            executable: /bin/bash
          environment:
            SPARK_MAJOR_VERSION : 2 #Run Job on Spark2

        - name: Wait until the string "completed" is in the file /tmp/foo before continuing
          wait_for:
            path: "{{ result_file }}"
            search_regex: 'Everything is working fine!'
      always:
        - name: Cleanup Job output and resources
          file: path="{{ item }}" state=absent
          with_items:
            - "{{ artifact_path }}"
            - "{{ result_file }}"
